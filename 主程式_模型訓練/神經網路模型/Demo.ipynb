{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s5KOYoITctVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562e9ef0-a919-40c8-b7d4-534196bff203"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/fNIRS_Deeplearning_Test\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "# path_dataset='./dataset/bandpass_30subjects/'\n",
        "path_dataset='./dataset/30subjects/'"
      ],
      "metadata": {
        "id": "EgX1GnXmd2NV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "path_to_this_work = '/content/drive/MyDrive/fNIRS_Deeplearning_Test'\n",
        "path_to_src = path_to_this_work + '/src'\n",
        "\n",
        "sys.path.insert(0, path_to_src)\n",
        "os.environ['PYTHONPATH'] += (\":\"+path_to_src)\n",
        "from BCI_model_class import SCCNet, SCCNet_25s, ShallowConvNet, ShallowConvNet2, cus_EEGNet\n",
        "import Dataset_training_schema as dts\n",
        "import train_model as tm"
      ],
      "metadata": {
        "id": "iVH_f3K-kSWa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7IHERDBLs3i",
        "outputId": "8d7c424b-d50f-46a6-b9f2-dae6348fb471"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from other_model import NIRS_CNN,NIRS_ANN, NIRS_LSTM, LSTM_getDataLoader\n",
        "from transformer import Residual, PreNorm, FeedForward, Attention, Transformer, PreBlock, fNIRS_T, LabelSmoothing, train_transformer"
      ],
      "metadata": {
        "id": "Kfm6bjO3LhEI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import scipy\n",
        "from scipy import io\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold"
      ],
      "metadata": {
        "id": "qh4Otv7ad4B6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "import torch\n",
        "import time"
      ],
      "metadata": {
        "id": "mwyXY12FmJZa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb -qqq\n",
        "# import wandb\n",
        "# wandb.login(key=\"550fa288f18a4938b6519ecd67d11309cfd9d51e\")"
      ],
      "metadata": {
        "id": "tiTh18Hi4Wg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "yRpWoAlLd5tR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb6164f-762b-43da-da22-07d5cf922f77"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader_model_train(test, train, config, model=0, rep_ID=\"\"):\n",
        "    channel_num = train.HbO.shape[1]\n",
        "    n_Hb=1\n",
        "\n",
        "    if config.Hb=='HbO':\n",
        "        print(\"Hb type : HbO\")\n",
        "        train_loader = dts.getDataLoader(train.HbO, train.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(test.HbO, test.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "    elif config.Hb=='HbR':\n",
        "        print(\"Hb type : HbR\")\n",
        "        train_loader = dts.getDataLoader(train.HbR, train.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(test.HbR, test.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "    elif config.Hb=='HbT':\n",
        "        print(\"Hb type : HbT\")\n",
        "        train_loader = dts.getDataLoader(train.HbT, train.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(test.HbT, test.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "    elif config.Hb=='HbO+HbR_40channel':\n",
        "        print(\"Hb type : HbO+HbR_40channel\")\n",
        "        Hb_test = np.concatenate((np.expand_dims(test.HbO,1), np.expand_dims(test.HbR,1)), axis=2)\n",
        "        Hb_train = np.concatenate((np.expand_dims(train.HbO,1), np.expand_dims(train.HbR,1)), axis=2)\n",
        "        train_loader = dts.getDataLoader(Hb_train, train.labels, batch_size=32, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(Hb_test, test.labels, batch_size=32, onehot_encoding=False)\n",
        "        channel_num = channel_num*2\n",
        "    elif config.Hb=='HbO+HbR_2layer':\n",
        "        print(\"Hb type : HbO+HbR_2layer\")\n",
        "        Hb_test = np.concatenate((np.expand_dims(test.HbO,1), np.expand_dims(test.HbR,1)), axis=1)\n",
        "        Hb_train = np.concatenate((np.expand_dims(train.HbO,1), np.expand_dims(train.HbR,1)), axis=1)\n",
        "        print(Hb_train.shape)\n",
        "        train_loader = dts.getDataLoader(Hb_train, train.labels, batch_size=32, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(Hb_test, test.labels, batch_size=32, onehot_encoding=False)\n",
        "        n_Hb=2\n",
        "\n",
        "    my_lr = config.learning_rate if model==0 else config.learning_rate/10\n",
        "    my_epoch = config.epochs if model==0 else math.ceil(config.epochs/10)\n",
        "    if model==0:\n",
        "      # model = cus_EEGNet(config.kernel_size)\n",
        "      if config.architecture==\"EEGNet\":\n",
        "          # model = cus_EEGNet(conv1_size=config.kernel_size, n_Hb=1, time_point=train_HbO.shape[2], channel_num =20,  acti_fun='elu')\n",
        "          model = cus_EEGNet(conv1_size=config.kernel_size, sep_conv_size=config.sep_conv_size, n_Hb=n_Hb, time_point=train.HbO.shape[2], channel_num=channel_num, acti_fun=config.acti_fun,  pooling_type = config.pooling_type, class_num=test.labels.shape[1], con1_type=config.con1_type)\n",
        "          model=model.to(device)\n",
        "          summary(model, (n_Hb, channel_num, train.HbO.shape[2]))\n",
        "      elif config.architecture==\"NIRS_CNN\":\n",
        "          model = NIRS_CNN(Hb_num=n_Hb, ch=train.HbO.shape[1], time_point=train.HbO.shape[2], class_num=train.labels.shape[1])\n",
        "          model=model.to(device)\n",
        "          print(\"channel_num: \",channel_num,\"-------------------------------------------------------------------------------------------------------------------------\")\n",
        "          print(\"train_HbO.shape[1]: \",train.HbO.shape[1],\"-------------------------------------------------------------------------------------------------------------------------\")\n",
        "          print(\"train_HbO.shape[2]: \",train.HbO.shape[2],\"-------------------------------------------------------------------------------------------------------------------------\")\n",
        "          summary(model, (n_Hb, channel_num, train.HbO.shape[2]))\n",
        "      elif config.architecture==\"NIRS_transformer\":\n",
        "          model = fNIRS_T(n_class=train.labels.shape[1], sampling_point= train.HbO.shape[2], dim=128, depth=6, heads=8, mlp_dim=64).to(device)\n",
        "      elif config.architecture==\"NIRS_ANN\": #*****************************************************************************************************************************\n",
        "          # train_HbO = np.squeeze(train_HbO)\n",
        "          # test_HbO = np.squeeze(test_HbO)\n",
        "          # train_loader = dts.getDataLoader(train_HbO, train_labels, batch_size=16, onehot_encoding=False)\n",
        "          # test_loader = dts.getDataLoader(test_HbO, test_labels, batch_size=16, onehot_encoding=False)\n",
        "          model = NIRS_ANN(Hb_num=n_Hb, ch=train.HbO.shape[1], time_point=train.HbO.shape[2], class_num=train.labels.shape[1])\n",
        "          model=model.to(device)\n",
        "          summary(model, (n_Hb, train.HbO.shape[1], train.HbO.shape[2]))\n",
        "          # summary(model, (1, train_HbO.shape[1]))\n",
        "      elif config.architecture==\"NIRS_LSTM\":\n",
        "          # model = FNIRSLSTM(input_size, hidden_size, num_classes)\n",
        "          model = NIRS_LSTM()\n",
        "          train_HbO_lstm = train.HbO.transpose(0, 2, 1)\n",
        "          test_HbO_lstm = test.HbO.transpose(0, 2, 1)\n",
        "          train_loader = LSTM_getDataLoader(train_HbO_lstm, train.labels, batch_size=8, onehot_encoding=False)\n",
        "          test_loader = LSTM_getDataLoader(test_HbO_lstm, test.labels, batch_size=8, onehot_encoding=False)\n",
        "      else:\n",
        "          if config.duration=='10s':\n",
        "              if config.architecture==\"SCCNet\":\n",
        "                  model = SCCNet(config.kernel_size)\n",
        "              elif config.architecture==\"ShallowConvNet\":\n",
        "                  model = ShallowConvNet()\n",
        "              model=model.to(device)\n",
        "              summary(model, (1, 20, 134))\n",
        "          elif config.duration=='25s':\n",
        "              if config.architecture==\"SCCNet\":\n",
        "                  model = SCCNet_25s(config.kernel_size)\n",
        "              elif config.architecture==\"ShallowConvNet\":\n",
        "                  model = ShallowConvNet()\n",
        "              model=model.to(device)\n",
        "              summary(model, (1, 20, 334))\n",
        "\n",
        "\n",
        "\n",
        "    if config.architecture==\"NIRS_transformer\":\n",
        "        return train_transformer(model,train_loader,test_loader,sub=config.sub,epoch=my_epoch,rep_ID = rep_ID)\n",
        "    else:\n",
        "        return tm.test_EEG_kernel(train_loader,test_loader,\n",
        "                model=model,\n",
        "                optimizer=config.optimizer,\n",
        "                # kernel_size=config.kernel_size,\n",
        "                epoch=my_epoch,\n",
        "                learning_rate=my_lr,\n",
        "                weight_decay=config.weight_decay,\n",
        "                wandb_import=False,\n",
        "                rep_ID = rep_ID)"
      ],
      "metadata": {
        "id": "KcNkWMMyeDsd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    def __init__(self,Hb,acti_fun,architecture,batch_size,con1_type,dataset,duration,epochs,kernel_size,learning_rate,optimizer,pooling_type,scheme,sep_conv_size,sub,weight_decay):\n",
        "        self.Hb = Hb\n",
        "        self.acti_fun = acti_fun\n",
        "        self.architecture = architecture\n",
        "        self.batch_size = batch_size\n",
        "        self.con1_type = con1_type\n",
        "        self.dataset = dataset\n",
        "        self.duration = duration\n",
        "        self.epochs = epochs\n",
        "        self.kernel_size = kernel_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.pooling_type = pooling_type\n",
        "        self.scheme = scheme\n",
        "        self.sep_conv_size = sep_conv_size\n",
        "        self.sub = sub\n",
        "        self.weight_decay = weight_decay"
      ],
      "metadata": {
        "id": "TBdIiX3Eh-z0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def train(config):\n",
        "  path_dataset='./dataset/'+ config.dataset + '_' + config.duration + '/'\n",
        "  s1=path_dataset+'S'\n",
        "  s2='.mat'\n",
        "\n",
        "\n",
        "\n",
        "  if config.scheme == 'SI' or config.scheme == 'SIFT': #------------------modify\n",
        "      if (config.dataset=='MA') | (config.dataset=='MI') | (config.dataset=='MA_raw') | (config.dataset=='MI_raw'):\n",
        "        # test_HbO, test_HbR, test_HbT, test_labels, train_HbO, train_HbR, train_HbT, train_labels = dts.leave_subject_out(s1,s2, data_range=range(1,30), test_id=int(config.sub))\n",
        "        test, train = dts.leave_subject_out(s1,s2, data_range=range(1,30), test_id=int(config.sub))\n",
        "      else:\n",
        "        test, train = dts.leave_subject_out(s1,s2, data_range=range(1,31), test_id=int(config.sub))\n",
        "  elif config.scheme == 'SD':\n",
        "      if (config.dataset=='MA') | (config.dataset=='MI') | (config.dataset=='MA_raw') | (config.dataset=='MI_raw'):\n",
        "        range1 = []\n",
        "        range2 = list(range(0,60,1))\n",
        "      else:\n",
        "        range1 = []\n",
        "        range2 = list(range(0,75,1))\n",
        "\n",
        "      test, train = dts.leave_trial_out(s1, s2, int(config.sub), range1, range2)\n",
        "  print(train.HbO.shape)\n",
        "\n",
        "  if config.scheme == 'SI' : #------------------modify\n",
        "      model, criterion, optimizer = dataloader_model_train(test, train, config)\n",
        "  elif config.scheme == 'SD':\n",
        "      skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)\n",
        "      skf.get_n_splits()\n",
        "      print(skf) # StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
        "      # get train set and test set index\n",
        "      for rep_ID, (train_index, test_index) in enumerate(skf.split(train.HbO, np.argmax(train.labels, axis=1))):\n",
        "          #print(\"train_index: \",train_index)\n",
        "          print(rep_ID)\n",
        "          print(\"test_index: \",test_index)\n",
        "          sd_test = dts.fnirs_data(HbO = train.HbO[test_index], HbR = train.HbR[test_index], HbT = train.HbT[test_index], labels = train.labels[test_index])\n",
        "          sd_train = dts.fnirs_data(HbO = train.HbO[train_index], HbR = train.HbR[train_index], HbT = train.HbT[train_index], labels = train.labels[train_index])\n",
        "          dataloader_model_train(sd_test, sd_train, config, rep_ID=str(rep_ID))\n",
        "  elif config.scheme == 'SIFT':\n",
        "      # model, criterion, optimizer = dataloader_model_train(test_HbO, test_HbR, test_HbT, test_labels, train_HbO, train_HbR, train_HbT, train_labels, config)\n",
        "      model, criterion, optimizer = dataloader_model_train(test, train, config)\n",
        "\n",
        "      skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)\n",
        "      skf.get_n_splits()\n",
        "      print(skf) # StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
        "      # get train set and test set index\n",
        "      for rep_ID, (val_index, trainFT_index) in enumerate(skf.split(test.HbO, np.argmax(test.labels, axis=1))):\n",
        "          print(\"trainFT_index: \",trainFT_index)\n",
        "          FT_trainFT = dts.fnirs_data(HbO = train.HbO[trainFT_index], HbR = train.HbR[trainFT_index], HbT = train.HbT[trainFT_index], labels = train.labels[trainFT_index])\n",
        "          FT_val = dts.fnirs_data(HbO = train.HbO[val_index], HbR = train.HbR[val_index], HbT = train.HbT[val_index], labels = train.labels[val_index])\n",
        "          dataloader_model_train(FT_val, FT_trainFT, config, model=model, rep_ID=str(rep_ID))\n",
        "\n",
        "\n",
        "\n",
        "  with open('finish_time.txt', 'a') as f:\n",
        "          seconds = time.time()\n",
        "          local_time = time.ctime(seconds)\n",
        "          f.write('FINISH!! ')\n",
        "          f.write(local_time)\n",
        "          f.write('\\n')\n",
        "\n",
        "  # if len(sys.argv)>=3:\n",
        "  #   projectName =  str(sys.argv[1])\n",
        "  #   i = 2\n",
        "  #   while i<len(sys.argv):\n",
        "  #     sweepID = str(sys.argv[i])\n",
        "  #     sweep_agent = 'cphnycu/' + projectName + '/' + sweepID\n",
        "  #     wandb.agent(sweep_agent, train)\n",
        "  #     #wandb.agent(sweep_agent, train, count=1)\n",
        "  #     i=i+1\n",
        "  # else:\n",
        "  #   print('empty')\n"
      ],
      "metadata": {
        "id": "4q9Dm4oEXken"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NIRS_ANN(nn.Module):\n",
        "    def __init__(self, Hb_num, ch, time_point, class_num):\n",
        "        super(NIRS_ANN, self).__init__()\n",
        "        input_dim = Hb_num * ch * time_point\n",
        "\n",
        "        self.fulcon1 = nn.Linear(input_dim, 20, bias=True)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.fulcon2 = nn.Linear(20, 10, bias=True)\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(10, class_num, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input: (batch_size, Hb_num * ch * time_point)\n",
        "        x = self.fulcon1(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.fulcon2(x)\n",
        "        x = self.act4(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "x-VBPP9YY8-7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NIRS_CNN(nn.Module):\n",
        "    # https://www.frontiersin.org/articles/10.3389/fnrgo.2023.994969/full\n",
        "    # Benchmarking framework for machine learning classification from fNIRS data\n",
        "    def __init__(self, Hb_num, ch, time_point=334, class_num=3):\n",
        "        super(NIRS_CNN, self).__init__()\n",
        "        # bs, 1, channel, sample\n",
        "        self.conv1 = nn.Conv2d(Hb_num, 4, (1, 10), padding=(0, 5),stride=(1, 2)) #更改\n",
        "        self.MaxPool1 = nn.MaxPool2d((1, 2))\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(4, 4, (1, 5), padding=(0, 2),stride=(1, 2)) #更改\n",
        "        self.MaxPool2 = nn.MaxPool2d((1, 2))\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.fulcon1 = nn.Linear(4*ch*math.ceil(time_point/16), 20, bias=True) # 更改\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.fulcon2 = nn.Linear(20, 10, bias=True) # 更改\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(10, class_num, bias=True) # 更改\n",
        "\n",
        "        self.linear = 4*ch*math.ceil(time_point/16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.MaxPool1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.MaxPool2(x)\n",
        "        x = x.view(-1, self.linear) # 更改\n",
        "        x = self.fulcon1(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.fulcon2(x)\n",
        "        x = self.act4(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        #x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iE8Tf4WioyFm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_config = config(\n",
        "  Hb = \"HbO+HbR_2layer\",\n",
        "  # Hb = \"HbO\",\n",
        "  acti_fun = \"elu\",\n",
        "  architecture = \"EEGNet\",\n",
        "  # architecture = \"NIRS_transformer\",\n",
        "  batch_size = 16,\n",
        "  con1_type = \"conv\",\n",
        "  dataset = \"MA\",\n",
        "  duration = \"25s\",\n",
        "  epochs = 10,\n",
        "  kernel_size = 64,\n",
        "  learning_rate = 0.001,\n",
        "  optimizer = \"adam\",\n",
        "  pooling_type = \"avgPool\",\n",
        "  scheme = \"SIFT\",\n",
        "  sep_conv_size = 16,\n",
        "  sub = \"1\",\n",
        "  weight_decay = 0.0001\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "UEt539nliELB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(my_config)"
      ],
      "metadata": {
        "id": "NXosoSnsjqci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for m in [\"NIRS_transformer\"]:\n",
        "# # for m in [\"NIRS_ANN\", \"NIRS_CNN\"]:\n",
        "# # for m in [\"EEGNet\",\"NIRS_transformer\", \"NIRS_ANN\", \"NIRS_CNN\"]:\n",
        "#   my_config.architecture=m\n",
        "#   for hb in [\"HbO+HbR_2layer\"]:\n",
        "#   # for hb in [\"HbO+HbR_2layer\",\"HbO\"]:\n",
        "#     my_config.Hb=hb\n",
        "#     # for d in [\"MA\"]:\n",
        "#     for d in [\"preprocess\",\"MA\"]:\n",
        "#       my_config.dataset=d\n",
        "#       for s in [\"SD\",\"SI\",\"SIFT\"]:\n",
        "#         my_config.scheme=s\n",
        "#         print(\"----------------------------------------------------\")\n",
        "#         print(\"Hb: \",hb)\n",
        "#         print(\"dataset: \",d)\n",
        "#         print(\"scheme: \",s)\n",
        "#         print(\"EEGNet: \",m)\n",
        "#         print(\"----------------------------------------------------\")\n",
        "#         train(my_config)"
      ],
      "metadata": {
        "id": "Y6997LmPkACn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}