{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s5KOYoITctVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5b7a81-7c78-40f8-a7f3-de13d7d3ccee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7IHERDBLs3i",
        "outputId": "e91811c2-d458-45e8-e3d7-4ad6d0c761bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#設定環境路徑"
      ],
      "metadata": {
        "id": "3KbSg9trzIRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "path_to_this_work = '/content/drive/MyDrive/fNIRS_Deeplearning_Test'\n",
        "os.chdir(path_to_this_work)\n",
        "os.listdir(path_to_this_work)\n",
        "path_to_src = path_to_this_work + '/src'\n",
        "\n",
        "sys.path.insert(0, path_to_src)\n",
        "os.environ['PYTHONPATH'] += (\":\"+path_to_src)\n"
      ],
      "metadata": {
        "id": "iVH_f3K-kSWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 引入套件"
      ],
      "metadata": {
        "id": "Q8qYKzpdzlV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from BCI_model_class import SCCNet, SCCNet_25s, ShallowConvNet, ShallowConvNet2, cus_EEGNet\n",
        "import Dataset_training_schema as dts\n",
        "import train_model as tm\n",
        "from other_model import NIRS_CNN,NIRS_ANN, NIRS_LSTM, LSTM_getDataLoader\n",
        "from transformer import Residual, PreNorm, FeedForward, Attention, Transformer, PreBlock, fNIRS_T, LabelSmoothing, train_transformer"
      ],
      "metadata": {
        "id": "Kfm6bjO3LhEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import scipy\n",
        "from scipy import io\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "import re\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold, StratifiedKFold"
      ],
      "metadata": {
        "id": "qh4Otv7ad4B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "import torch\n",
        "import time"
      ],
      "metadata": {
        "id": "mwyXY12FmJZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "yRpWoAlLd5tR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7fa2654-68cc-450a-c599-afe16df9f51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 方法"
      ],
      "metadata": {
        "id": "huM5RKHAz5Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader_model_train(test, train, config, model=0, rep_ID=\"\"):\n",
        "    channel_num = train.HbO.shape[1]\n",
        "    n_Hb=1\n",
        "\n",
        "    if config.Hb=='HbO':\n",
        "        print(\"Hb type : HbO\")\n",
        "        train_loader = dts.getDataLoader(train.HbO, train.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(test.HbO, test.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "    elif config.Hb=='HbR':\n",
        "        print(\"Hb type : HbR\")\n",
        "        train_loader = dts.getDataLoader(train.HbR, train.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(test.HbR, test.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "    elif config.Hb=='HbT':\n",
        "        print(\"Hb type : HbT\")\n",
        "        train_loader = dts.getDataLoader(train.HbT, train.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(test.HbT, test.labels, batch_size=config.batch_size, onehot_encoding=False)\n",
        "    elif config.Hb=='HbO+HbR_40channel':\n",
        "        print(\"Hb type : HbO+HbR_40channel\")\n",
        "        Hb_test = np.concatenate((np.expand_dims(test.HbO,1), np.expand_dims(test.HbR,1)), axis=2)\n",
        "        Hb_train = np.concatenate((np.expand_dims(train.HbO,1), np.expand_dims(train.HbR,1)), axis=2)\n",
        "        train_loader = dts.getDataLoader(Hb_train, train.labels, batch_size=32, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(Hb_test, test.labels, batch_size=32, onehot_encoding=False)\n",
        "        channel_num = channel_num*2\n",
        "    elif config.Hb=='HbO+HbR_2layer':\n",
        "        print(\"Hb type : HbO+HbR_2layer\")\n",
        "        Hb_test = np.concatenate((np.expand_dims(test.HbO,1), np.expand_dims(test.HbR,1)), axis=1)\n",
        "        Hb_train = np.concatenate((np.expand_dims(train.HbO,1), np.expand_dims(train.HbR,1)), axis=1)\n",
        "        print(Hb_train.shape)\n",
        "        train_loader = dts.getDataLoader(Hb_train, train.labels, batch_size=32, onehot_encoding=False)\n",
        "        test_loader = dts.getDataLoader(Hb_test, test.labels, batch_size=32, onehot_encoding=False)\n",
        "        n_Hb=2\n",
        "\n",
        "    my_lr = config.learning_rate if model==0 else config.learning_rate/10\n",
        "    my_epoch = config.epochs if model==0 else math.ceil(config.epochs/10)\n",
        "    if model==0:\n",
        "      # model = cus_EEGNet(config.kernel_size)\n",
        "      if config.architecture==\"EEGNet\":\n",
        "          # model = cus_EEGNet(conv1_size=config.kernel_size, n_Hb=1, time_point=train_HbO.shape[2], channel_num =20,  acti_fun='elu')\n",
        "          model = cus_EEGNet(conv1_size=config.kernel_size, sep_conv_size=config.sep_conv_size, n_Hb=n_Hb, time_point=train.HbO.shape[2], channel_num=channel_num, acti_fun=config.acti_fun,  pooling_type = config.pooling_type, class_num=test.labels.shape[1], con1_type=config.con1_type)\n",
        "          model=model.to(device)\n",
        "          summary(model, (n_Hb, channel_num, train.HbO.shape[2]))\n",
        "      elif config.architecture==\"NIRS_CNN\":\n",
        "          model = NIRS_CNN(Hb_num=n_Hb, ch=train.HbO.shape[1], time_point=train.HbO.shape[2], class_num=train.labels.shape[1])\n",
        "          model=model.to(device)\n",
        "          print(\"channel_num: \",channel_num,\"-------------------------------------------------------------------------------------------------------------------------\")\n",
        "          print(\"train_HbO.shape[1]: \",train.HbO.shape[1],\"-------------------------------------------------------------------------------------------------------------------------\")\n",
        "          print(\"train_HbO.shape[2]: \",train.HbO.shape[2],\"-------------------------------------------------------------------------------------------------------------------------\")\n",
        "          summary(model, (n_Hb, channel_num, train.HbO.shape[2]))\n",
        "      elif config.architecture==\"NIRS_transformer\":\n",
        "          model = fNIRS_T(n_class=train.labels.shape[1], sampling_point= train.HbO.shape[2], dim=128, depth=6, heads=8, mlp_dim=64).to(device)\n",
        "      elif config.architecture==\"NIRS_ANN\": #*****************************************************************************************************************************\n",
        "          # train_HbO = np.squeeze(train_HbO)\n",
        "          # test_HbO = np.squeeze(test_HbO)\n",
        "          # train_loader = dts.getDataLoader(train_HbO, train_labels, batch_size=16, onehot_encoding=False)\n",
        "          # test_loader = dts.getDataLoader(test_HbO, test_labels, batch_size=16, onehot_encoding=False)\n",
        "          model = NIRS_ANN(Hb_num=n_Hb, ch=train.HbO.shape[1], time_point=train.HbO.shape[2], class_num=train.labels.shape[1])\n",
        "          model=model.to(device)\n",
        "          summary(model, (n_Hb, train.HbO.shape[1], train.HbO.shape[2]))\n",
        "          # summary(model, (1, train_HbO.shape[1]))\n",
        "      elif config.architecture==\"NIRS_LSTM\":\n",
        "          # model = FNIRSLSTM(input_size, hidden_size, num_classes)\n",
        "          model = NIRS_LSTM()\n",
        "          train_HbO_lstm = train.HbO.transpose(0, 2, 1)\n",
        "          test_HbO_lstm = test.HbO.transpose(0, 2, 1)\n",
        "          train_loader = LSTM_getDataLoader(train_HbO_lstm, train.labels, batch_size=8, onehot_encoding=False)\n",
        "          test_loader = LSTM_getDataLoader(test_HbO_lstm, test.labels, batch_size=8, onehot_encoding=False)\n",
        "      else:\n",
        "          if config.duration=='10s':\n",
        "              if config.architecture==\"SCCNet\":\n",
        "                  model = SCCNet(config.kernel_size)\n",
        "              elif config.architecture==\"ShallowConvNet\":\n",
        "                  model = ShallowConvNet()\n",
        "              model=model.to(device)\n",
        "              summary(model, (1, 20, 134))\n",
        "          elif config.duration=='25s':\n",
        "              if config.architecture==\"SCCNet\":\n",
        "                  model = SCCNet_25s(config.kernel_size)\n",
        "              elif config.architecture==\"ShallowConvNet\":\n",
        "                  model = ShallowConvNet()\n",
        "              model=model.to(device)\n",
        "              summary(model, (1, 20, 334))\n",
        "\n",
        "\n",
        "\n",
        "    if config.architecture==\"NIRS_transformer\":\n",
        "        return train_transformer(model,train_loader,test_loader,sub=config.sub,epoch=my_epoch,rep_ID = rep_ID)\n",
        "    else:\n",
        "        return tm.test_EEG_kernel(train_loader,test_loader,\n",
        "                model=model,\n",
        "                optimizer=config.optimizer,\n",
        "                # kernel_size=config.kernel_size,\n",
        "                epoch=my_epoch,\n",
        "                learning_rate=my_lr,\n",
        "                weight_decay=config.weight_decay,\n",
        "                wandb_import=False,\n",
        "                rep_ID = rep_ID)"
      ],
      "metadata": {
        "id": "KcNkWMMyeDsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    def __init__(self,Hb,acti_fun,architecture,batch_size,con1_type,dataset,duration,epochs,kernel_size,learning_rate,optimizer,pooling_type,scheme,sep_conv_size,sub,weight_decay):\n",
        "        self.Hb = Hb\n",
        "        self.acti_fun = acti_fun\n",
        "        self.architecture = architecture\n",
        "        self.batch_size = batch_size\n",
        "        self.con1_type = con1_type\n",
        "        self.dataset = dataset\n",
        "        self.duration = duration\n",
        "        self.epochs = epochs\n",
        "        self.kernel_size = kernel_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optimizer\n",
        "        self.pooling_type = pooling_type\n",
        "        self.scheme = scheme\n",
        "        self.sep_conv_size = sep_conv_size\n",
        "        self.sub = sub\n",
        "        self.weight_decay = weight_decay"
      ],
      "metadata": {
        "id": "TBdIiX3Eh-z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def train(config):\n",
        "  path_dataset='./dataset/'+ config.dataset + '_' + config.duration + '/'\n",
        "  s1=path_dataset+'S'\n",
        "  s2='.mat'\n",
        "\n",
        "\n",
        "\n",
        "  if config.scheme == 'SI' or config.scheme == 'SIFT': #------------------modify\n",
        "      if (config.dataset=='MA') | (config.dataset=='MI') | (config.dataset=='MA_raw') | (config.dataset=='MI_raw'):\n",
        "        test, train = dts.leave_subject_out(s1,s2, data_range=range(1,30), test_id=int(config.sub))\n",
        "      else:\n",
        "        test, train = dts.leave_subject_out(s1,s2, data_range=range(1,31), test_id=int(config.sub))\n",
        "  elif config.scheme == 'SD':\n",
        "      if (config.dataset=='MA') | (config.dataset=='MI') | (config.dataset=='MA_raw') | (config.dataset=='MI_raw'):\n",
        "        range1 = []\n",
        "        range2 = list(range(0,60,1))\n",
        "      else:\n",
        "        range1 = []\n",
        "        range2 = list(range(0,75,1))\n",
        "\n",
        "      test, train = dts.leave_trial_out(s1, s2, int(config.sub), range1, range2)\n",
        "  print(train.HbO.shape)\n",
        "\n",
        "  if config.scheme == 'SI' : #------------------modify\n",
        "      model, criterion, optimizer = dataloader_model_train(test, train, config)\n",
        "  elif config.scheme == 'SD':\n",
        "      skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)\n",
        "      skf.get_n_splits()\n",
        "      print(skf) # StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
        "      # get train set and test set index\n",
        "      for rep_ID, (train_index, test_index) in enumerate(skf.split(train.HbO, np.argmax(train.labels, axis=1))):\n",
        "          #print(\"train_index: \",train_index)\n",
        "          print(rep_ID)\n",
        "          print(\"test_index: \",test_index)\n",
        "          sd_test = dts.fnirs_data(HbO = train.HbO[test_index], HbR = train.HbR[test_index], HbT = train.HbT[test_index], labels = train.labels[test_index])\n",
        "          sd_train = dts.fnirs_data(HbO = train.HbO[train_index], HbR = train.HbR[train_index], HbT = train.HbT[train_index], labels = train.labels[train_index])\n",
        "          dataloader_model_train(sd_test, sd_train, config, rep_ID=str(rep_ID))\n",
        "  elif config.scheme == 'SIFT':\n",
        "      model, criterion, optimizer = dataloader_model_train(test, train, config)\n",
        "\n",
        "      skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)\n",
        "      skf.get_n_splits()\n",
        "      print(skf) # StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
        "      # get train set and test set index\n",
        "      for rep_ID, (val_index, trainFT_index) in enumerate(skf.split(test.HbO, np.argmax(test.labels, axis=1))):\n",
        "          print(\"trainFT_index: \",trainFT_index)\n",
        "          FT_trainFT = dts.fnirs_data(HbO = train.HbO[trainFT_index], HbR = train.HbR[trainFT_index], HbT = train.HbT[trainFT_index], labels = train.labels[trainFT_index])\n",
        "          FT_val = dts.fnirs_data(HbO = train.HbO[val_index], HbR = train.HbR[val_index], HbT = train.HbT[val_index], labels = train.labels[val_index])\n",
        "          dataloader_model_train(FT_val, FT_trainFT, config, model=model, rep_ID=str(rep_ID))\n",
        "\n",
        "\n",
        "\n",
        "  with open('finish_time.txt', 'a') as f:\n",
        "          seconds = time.time()\n",
        "          local_time = time.ctime(seconds)\n",
        "          f.write('FINISH!! ')\n",
        "          f.write(local_time)\n",
        "          f.write('\\n')\n"
      ],
      "metadata": {
        "id": "4q9Dm4oEXken"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NIRS_ANN(nn.Module):\n",
        "    def __init__(self, Hb_num, ch, time_point, class_num):\n",
        "        super(NIRS_ANN, self).__init__()\n",
        "        input_dim = Hb_num * ch * time_point\n",
        "\n",
        "        self.fulcon1 = nn.Linear(input_dim, 20, bias=True)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.fulcon2 = nn.Linear(20, 10, bias=True)\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(10, class_num, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input: (batch_size, Hb_num * ch * time_point)\n",
        "        x = self.fulcon1(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.fulcon2(x)\n",
        "        x = self.act4(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "x-VBPP9YY8-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NIRS_CNN(nn.Module):\n",
        "    # https://www.frontiersin.org/articles/10.3389/fnrgo.2023.994969/full\n",
        "    # Benchmarking framework for machine learning classification from fNIRS data\n",
        "    def __init__(self, Hb_num, ch, time_point=334, class_num=3):\n",
        "        super(NIRS_CNN, self).__init__()\n",
        "        # bs, 1, channel, sample\n",
        "        self.conv1 = nn.Conv2d(Hb_num, 4, (1, 10), padding=(0, 5),stride=(1, 2)) #更改\n",
        "        self.MaxPool1 = nn.MaxPool2d((1, 2))\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(4, 4, (1, 5), padding=(0, 2),stride=(1, 2)) #更改\n",
        "        self.MaxPool2 = nn.MaxPool2d((1, 2))\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.fulcon1 = nn.Linear(4*ch*math.ceil(time_point/16), 20, bias=True) # 更改\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.fulcon2 = nn.Linear(20, 10, bias=True) # 更改\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.classifier = nn.Linear(10, class_num, bias=True) # 更改\n",
        "\n",
        "        self.linear = 4*ch*math.ceil(time_point/16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.MaxPool1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.MaxPool2(x)\n",
        "        x = x.view(-1, self.linear) # 更改\n",
        "        x = self.fulcon1(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.fulcon2(x)\n",
        "        x = self.act4(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        #x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iE8Tf4WioyFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#設定套件"
      ],
      "metadata": {
        "id": "ZohQDeuSz863"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_config = config(\n",
        "  Hb = \"HbO+HbR_2layer\",\n",
        "  acti_fun = \"elu\",\n",
        "  architecture = \"EEGNet\",\n",
        "  # architecture = \"NIRS_transformer\",\n",
        "  batch_size = 16,\n",
        "  con1_type = \"conv\",\n",
        "  dataset = \"MA\",\n",
        "  duration = \"25s\",\n",
        "  epochs = 10,\n",
        "  kernel_size = 64,\n",
        "  learning_rate = 0.001,\n",
        "  optimizer = \"adam\",\n",
        "  pooling_type = \"avgPool\",\n",
        "  scheme = \"SIFT\",\n",
        "  sep_conv_size = 16,\n",
        "  sub = \"1\",\n",
        "  weight_decay = 0.0001\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "UEt539nliELB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(my_config)"
      ],
      "metadata": {
        "id": "NXosoSnsjqci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb1fd65-80cb-4be5-9a23-5d99dd2b0fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leave subject out\n",
            "train id:2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, test id:  1\n",
            "training data dimension:  (1680, 36, 251)\n",
            "training label dimension:  (1680, 2)\n",
            "testing data dimension:  (60, 36, 251)\n",
            "testing label dimension:  (60, 2)\n",
            "(1680, 36, 251)\n",
            "Hb type : HbO+HbR_2layer\n",
            "(1680, 2, 36, 251)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 16, 36, 252]           2,048\n",
            "       BatchNorm2d-2          [-1, 16, 36, 252]              32\n",
            "            Conv2d-3           [-1, 32, 1, 252]           1,152\n",
            "       BatchNorm2d-4           [-1, 32, 1, 252]              64\n",
            "               ELU-5           [-1, 32, 1, 252]               0\n",
            "         AvgPool2d-6            [-1, 32, 1, 63]               0\n",
            "           Dropout-7            [-1, 32, 1, 63]               0\n",
            "            Conv2d-8            [-1, 32, 1, 64]             512\n",
            "            Conv2d-9            [-1, 32, 1, 64]           1,024\n",
            "      BatchNorm2d-10            [-1, 32, 1, 64]              64\n",
            "              ELU-11            [-1, 32, 1, 64]               0\n",
            "        AvgPool2d-12             [-1, 32, 1, 8]               0\n",
            "          Dropout-13             [-1, 32, 1, 8]               0\n",
            "           Linear-14                    [-1, 2]             514\n",
            "================================================================\n",
            "Total params: 5,410\n",
            "Trainable params: 5,410\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.07\n",
            "Forward/backward pass size (MB): 2.50\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 2.59\n",
            "----------------------------------------------------------------\n",
            "cuda:0\n",
            "epoch, train, test, train_loss, test_loss\n",
            "0, 50.00%, 50.00%, 0.02181, 0.02310\n",
            "1, 71.55%, 51.67%, 0.01765, 0.02428\n",
            "2, 74.35%, 61.67%, 0.01621, 0.02552\n",
            "3, 76.37%, 58.33%, 0.01569, 0.02558\n",
            "4, 78.27%, 61.67%, 0.01489, 0.02454\n",
            "5, 79.05%, 66.67%, 0.01428, 0.02704\n",
            "6, 77.68%, 61.67%, 0.01490, 0.02698\n",
            "7, 80.65%, 61.67%, 0.01352, 0.02697\n",
            "8, 80.54%, 66.67%, 0.01356, 0.02861\n",
            "9, 81.43%, 61.67%, 0.01327, 0.02660\n",
            "StratifiedKFold(n_splits=3, random_state=None, shuffle=True)\n",
            "trainFT_index:  [ 8  9 16 24 29 32 33 35 36 37 38 39 44 47 51 52 55 56 57 58]\n",
            "Hb type : HbO+HbR_2layer\n",
            "(20, 2, 36, 251)\n",
            "cuda:0\n",
            "epoch, train, test, train_loss, test_loss\n",
            "0, 85.00%, 75.00%, 0.01666, 0.02500\n",
            "trainFT_index:  [ 6 11 14 15 18 20 21 22 23 26 27 30 31 40 41 42 45 48 49 50]\n",
            "Hb type : HbO+HbR_2layer\n",
            "(20, 2, 36, 251)\n",
            "cuda:0\n",
            "epoch, train, test, train_loss, test_loss\n",
            "0, 75.00%, 80.00%, 0.02939, 0.01657\n",
            "trainFT_index:  [ 0  1  2  3  4  5  7 10 12 13 17 19 25 28 34 43 46 53 54 59]\n",
            "Hb type : HbO+HbR_2layer\n",
            "(20, 2, 36, 251)\n",
            "cuda:0\n",
            "epoch, train, test, train_loss, test_loss\n",
            "0, 75.00%, 80.00%, 0.01950, 0.01815\n"
          ]
        }
      ]
    }
  ]
}